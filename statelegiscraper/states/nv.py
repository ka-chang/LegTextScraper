"""
NV module for scraping and processing text from https://leg.state.nv.us

# Status, as of January 1, 2022

Current Coverage (Stable):
    [X] Committee Hearings (PDF Transcript Links) (2011 - 2021)

Planned Coverage:
    [ ] Floor Speeches

# NV Work Flow

StateLegiscraper has two classes for each state module: Scrape and Process

    Scrape includes 1 function that scrapes PDF transcripts present at a list of weblinks
    This class provides user with raw data saved on a local or mounted drive

    Process includes 2 functions that processes raw data (PDF Transcripts)
    generated by Scrape class functions.
    This class provides useres with Python objects ready to work with popular
    NLP packages (e.g., nltk, SpaCy)

CLASS Scrape

    - nv_scrape_pdf

CLASS Process

    - nv_pdf_to_text
    - nv_text_clean

"""

import json
import os
import re
import time
import urllib.request

#import string
#from urllib.parse import urljoin

import pdfplumber
from selenium import webdriver
from selenium.webdriver.chrome.service import Service


class Scrape:
    """
    Scrape functions for Nevada State Legislature website
    """

    def nv_scrape_pdf(webscrape_links, dir_chrome_webdriver, dir_save):
        """
        Webscrape function for Nevada State Legislature Website.

        Parameters
        ----------
        webscrape_links : List
            List of direct link(s) to NV committee webpage.
            see assets/weblinks/nv_weblinks.py for lists organized by chamber and committee
        dir_chrome_webdriver : String
            Local directory that contains the appropriate Chrome Webdriver.
        dir_save : String
            Local directory to save PDFs.

        Returns
        -------
        All PDF files found on the webscrape_links, saved on local dir_save.

        """

        if not isinstance(webscrape_links, list):
            raise ValueError("webscrape_links must be a list")
        else:
            pass

        if not os.path.exists(dir_chrome_webdriver):
            raise ValueError("Chrome Webdriver not found")
        else:
            pass

        if not os.path.exists(dir_save):
            raise ValueError("Save directory not found")
        else:
            pass

        for link_index in range(len(webscrape_links)):

            service = Service(dir_chrome_webdriver)
            options = webdriver.ChromeOptions()
            # Chrome runs headless, 
            # comment out "options.add_argument('headless')"
            # to see the action
            options.add_argument('headless')
            driver = webdriver.Chrome(service=service, options=options)

            time.sleep(5)
            driver.get(webscrape_links[link_index])
            time.sleep(5)

            arrow01 = driver.find_element_by_id('divCommitteePageMeetings')
            arrow01.click()
            time.sleep(5)

            arrow02 = driver.find_element_by_id('divMeetings')
            arrow02.click()

            url = driver.page_source
            regex_pattern = r'https.*Minutes.*\.pdf'
            lines = url.split()
            meeting_regex = re.compile(regex_pattern)
            all_files = []

            for line in lines:
                hit = meeting_regex.findall(line)
                if hit:
                    all_files.extend(hit)

            for filename in all_files:
                print(filename)

            folder_location = dir_save

            for link in all_files:
                filename = os.path.join(
                    folder_location, "_".join(link.split('/')[4:]))
                urllib.request.urlretrieve(link, filename)
            time.sleep(15)

            driver.close()


class Process:
    """
    Process functions for PDF transcripts scraped from Nevada State Legislature website
    """

    def nv_pdf_to_text(dir_load, nv_json_name):
        """
        Convert all PDFs to a dictionary and then saved locally as a JSON file.

        Parameters
        ----------
        dir_load : String
            Local location of the directory holding PDFs.
        nv_json_name : String
            JSON file name, include full local path.

        Returns
        -------
        A single JSON file, can be loaded as dictionary to work with.

        """
        directory = dir_load
        n = 0
        committee = {}

        file_list = sorted(os.listdir(directory))
        del file_list[0]

        for file in file_list:
            filename = directory + file
            all_text = ''
            with pdfplumber.open(filename) as pdf:
                for pdf_page in pdf.pages:
                    single_page_text = pdf_page.extract_text()
                    all_text = all_text + '\n' + single_page_text
                    committee[n] = all_text
            n = n + 1

        with open(nv_json_name, 'w') as file:
            json.dump(committee, file, ensure_ascii=False)

    def nv_text_clean(nv_json_path, trim=None):
        """
        Loads JSON into environment as dictionary
        Preprocesses the raw PDF export from previously generated json
        Optional: Trims transcript to exclude list of those present
        and signature page/list of exhibits

        Parameters
        ----------
        nv_json_path : String
            Local path of nv_json generated by nv_pdf_to_text.
        trim: True/Default(None)
            Provides option to trim transcript to spoken section and transcriber notes

        Returns
        -------
        Cleaned dictionary that excludes PDF formatting and (optional) front and back end

        """

        file_path = open(nv_json_path,)
        data = json.load(file_path)

        if trim:
            for key in data:
                if isinstance(data[key], str):
                    transcript = data[key]
                    start_location = re.search(
                        r"(CHAIR.*[A-z]\:|Chair.*[A-z]\:)", transcript).start()
                    # Starts transcript from when Chair first speaks
                    transcript = transcript[start_location:]
                    # Removes signature page after submission (RESPECTFULLY
                    # SUBMITTED)
                    end_location = re.search(
                        r"(Respectfully\sSUBMITTED\:|RESPECTFULLY\sSUBMITTED\:|RESPECTFULLY\sSUBMITTED)",
                        transcript)
                    if end_location:
                        end_location_num = end_location.start()
                        transcript = transcript[:end_location_num]
                    transcript = re.sub(
                        r"Page\s[0-9]{1,}", "", transcript)  # Removes page number
                    transcript = re.sub(r"\n", "", transcript)
                    transcript = transcript.strip()
                    transcript = " ".join(transcript.split())
                    data[key] = transcript
                else:
                    print("Incompatible File")

            return data

        else:
            for key in data:
                if isinstance(data[key], str):
                    transcript = data[key]
                    transcript = re.sub(r"Page\s[0-9]{1,}", "", transcript)
                    transcript = re.sub(r"\n", "", transcript)
                    transcript = transcript.strip()
                    transcript = " ".join(transcript.split())
                    data[key] = transcript
                else:
                    print("Incompatible File")

            return data
